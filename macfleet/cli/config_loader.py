"""YAML configuration loader for MacFleet.

Allows users to configure MacFleet via a YAML file instead of
writing Python code directly.

Example macfleet.yaml:
    cluster:
      role: master           # or worker
      master_addr: 10.0.0.1  # IP of master node
      master_port: 50051
      tensor_port: 50052
      host: null             # override bind IP, or null for auto-detect
      min_workers: 1
      discovery_enabled: true

    training:
      epochs: 10
      batch_size: 128
      learning_rate: 0.1
      compression: topk_fp16   # none | topk | fp16 | topk_fp16
      topk_ratio: 0.1
      device: mps              # mps | cpu | cuda
      checkpoint_every: 5
      checkpoint_dir: ./checkpoints
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Optional

from macfleet.core.config import ClusterConfig, CompressionType, NodeRole, TrainingConfig


def load_yaml_config(path: str) -> dict:
    """Load and parse a YAML configuration file.

    Args:
        path: Path to the YAML config file.

    Returns:
        Parsed configuration dictionary.

    Raises:
        FileNotFoundError: If the file does not exist.
        ImportError: If PyYAML is not installed.
        ValueError: If the YAML is malformed or missing required fields.
    """
    if not os.path.exists(path):
        raise FileNotFoundError(
            f"Config file not found: {path}\n"
            "Run 'macfleet init' to create a starter config file."
        )

    try:
        import yaml
    except ImportError:
        raise ImportError(
            "PyYAML is required for config file support.\n"
            "Install it with: pip install pyyaml"
        )

    with open(path) as f:
        try:
            data = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML in {path}: {e}")

    if data is None:
        raise ValueError(f"Empty config file: {path}")

    return data


def cluster_config_from_yaml(path: str, role_override: Optional[str] = None) -> ClusterConfig:
    """Create a ClusterConfig from a YAML file.

    Args:
        path: Path to the YAML config file.
        role_override: Override the role from the file (useful for CLI flags).

    Returns:
        ClusterConfig instance.
    """
    data = load_yaml_config(path)
    cluster_data = data.get("cluster", {})

    # Determine role
    role_str = role_override or cluster_data.get("role", "master")
    try:
        role = NodeRole(role_str)
    except ValueError:
        raise ValueError(
            f"Invalid role '{role_str}'. Must be 'master' or 'worker'."
        )

    return ClusterConfig(
        role=role,
        master_addr=cluster_data.get("master_addr", ""),
        master_port=int(cluster_data.get("master_port", 50051)),
        tensor_port=int(cluster_data.get("tensor_port", 50052)),
        host=cluster_data.get("host") or None,
        min_workers=int(cluster_data.get("min_workers", 1)),
        discovery_enabled=bool(cluster_data.get("discovery_enabled", True)),
        heartbeat_interval_sec=float(cluster_data.get("heartbeat_interval_sec", 2.0)),
        heartbeat_timeout_sec=float(cluster_data.get("heartbeat_timeout_sec", 6.0)),
    )


def training_config_from_yaml(path: str) -> TrainingConfig:
    """Create a TrainingConfig from a YAML file.

    Args:
        path: Path to the YAML config file.

    Returns:
        TrainingConfig instance.
    """
    data = load_yaml_config(path)
    training_data = data.get("training", {})

    compression_str = training_data.get("compression", "topk_fp16")
    try:
        compression = CompressionType(compression_str)
    except ValueError:
        valid = [c.value for c in CompressionType]
        raise ValueError(
            f"Invalid compression '{compression_str}'. Must be one of: {valid}"
        )

    return TrainingConfig(
        epochs=int(training_data.get("epochs", 10)),
        batch_size=int(training_data.get("batch_size", 128)),
        learning_rate=float(training_data.get("learning_rate", 0.1)),
        compression=compression,
        topk_ratio=float(training_data.get("topk_ratio", 0.1)),
        checkpoint_every=int(training_data.get("checkpoint_every", 5)),
        checkpoint_dir=training_data.get("checkpoint_dir", "./checkpoints"),
        calibration_steps=int(training_data.get("calibration_steps", 10)),
        dynamic_rebalance_threshold=float(
            training_data.get("dynamic_rebalance_threshold", 0.15)
        ),
        device=training_data.get("device", "mps"),
    )


# ── Template strings ──────────────────────────────────────────────────────────

_YAML_TEMPLATE = """\
# MacFleet configuration
# Generated by: macfleet init
#
# Edit this file then launch with:
#   macfleet launch --config macfleet.yaml --role master
#   macfleet launch --config macfleet.yaml --role worker

cluster:
  # Role of this node: master or worker
  # Override at launch time with --role flag
  role: master

  # IP address of the master node
  # Set this on both master and worker configs
  master_addr: 10.0.0.1

  # gRPC control port (default: 50051)
  master_port: 50051

  # TCP tensor transfer port (default: 50052)
  tensor_port: 50052

  # IP address to bind to on this machine
  # null = auto-detect (prefers Thunderbolt bridge IP)
  # Set explicitly if auto-detect picks the wrong interface
  host: null

  # Minimum workers required before training starts
  min_workers: 1

  # Enable Bonjour/zeroconf auto-discovery
  discovery_enabled: true

training:
  # Number of training epochs
  epochs: 10

  # Total batch size (split across nodes proportionally)
  batch_size: 128

  # Initial learning rate
  learning_rate: 0.1

  # Gradient compression: none | topk | fp16 | topk_fp16
  # topk_fp16 gives ~20x compression (recommended for Thunderbolt)
  compression: topk_fp16

  # Top-K ratio for sparsification (0.0-1.0)
  # Lower = more compression, less accuracy
  topk_ratio: 0.1

  # PyTorch device: mps (Apple Silicon GPU) | cpu | cuda
  device: mps

  # Save checkpoint every N epochs
  checkpoint_every: 5

  # Directory for saving checkpoints
  checkpoint_dir: ./checkpoints
"""

_TRAIN_TEMPLATE = '''\
#!/usr/bin/env python3
"""Custom training script using MacFleet distributed training.

Usage:
    # Single node (for testing):
    python train.py

    # Distributed (master Mac):
    python train.py --distributed --role master

    # Distributed (worker Mac):
    python train.py --distributed --role worker
"""

import argparse
import torch
import torch.nn as nn
from torch.utils.data import Dataset

from macfleet import ClusterConfig, NodeRole, TrainingConfig
from macfleet.training.trainer import Trainer


# ─── Define your model ───────────────────────────────────────────────────────

class MyModel(nn.Module):
    """Replace this with your actual model."""

    def __init__(self, num_classes: int = 10):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        return self.layers(x.view(x.size(0), -1))


# ─── Define your dataset ─────────────────────────────────────────────────────

class MyDataset(Dataset):
    """Replace this with your actual dataset."""

    def __init__(self, num_samples: int = 1000, num_classes: int = 10):
        self.data = torch.randn(num_samples, 1, 28, 28)
        self.labels = torch.randint(0, num_classes, (num_samples,))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]


# ─── Main ─────────────────────────────────────────────────────────────────────

def main():
    parser = argparse.ArgumentParser(description="Train with MacFleet")
    parser.add_argument("--config", type=str, default=None,
                        help="Path to macfleet.yaml config file")
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch-size", type=int, default=128)
    parser.add_argument("--lr", type=float, default=0.01)
    parser.add_argument("--distributed", action="store_true")
    parser.add_argument("--role", choices=["master", "worker"], default="master")
    parser.add_argument("--master", type=str, default="10.0.0.1")
    parser.add_argument("--host", type=str, default=None)
    args = parser.parse_args()

    # Load from YAML config if provided, otherwise use CLI args
    if args.config:
        from macfleet.cli.config_loader import cluster_config_from_yaml, training_config_from_yaml
        cluster_config = cluster_config_from_yaml(args.config, role_override=args.role)
        training_config = training_config_from_yaml(args.config)
    else:
        cluster_config = ClusterConfig(
            role=NodeRole.MASTER if args.role == "master" else NodeRole.WORKER,
            master_addr=args.master,
            host=args.host,
        )
        training_config = TrainingConfig(
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr,
            device="mps" if torch.backends.mps.is_available() else "cpu",
        )

    # ── Create your model and dataset ─────────────────────────────────────
    model = MyModel(num_classes=10)
    train_dataset = MyDataset(num_samples=5000)
    val_dataset = MyDataset(num_samples=1000)

    num_params = sum(p.numel() for p in model.parameters())
    print(f"Model parameters: {num_params:,}")
    print(f"Train samples: {len(train_dataset)}")
    # ──────────────────────────────────────────────────────────────────────

    trainer = Trainer(
        model=model,
        dataset=train_dataset,
        training_config=training_config,
        cluster_config=cluster_config,
        optimizer_cls=torch.optim.Adam,
        optimizer_kwargs={"lr": training_config.learning_rate},
        criterion=nn.CrossEntropyLoss(),
        val_dataset=val_dataset,
        distributed=args.distributed,
    )

    state = trainer.fit()
    print(f"Training complete! Final epoch: {state.epoch + 1}, Steps: {state.global_step}")


if __name__ == "__main__":
    main()
'''
